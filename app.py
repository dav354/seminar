#!/usr/bin/env python3
import cv2
import time
import numpy as np
import mediapipe as mp
from flask import Flask, Response
from camera import setup_camera
from draw import draw_landmarks
from tflite_runtime.interpreter import Interpreter, load_delegate
import sys

print("[üöÄ] Booting Flask app...")

# === Load Edge TPU Model ===
try:
    print("[üì¶] Loading Edge TPU model from models/model_edgetpu.tflite...")
    interpreter = Interpreter(
        model_path="models/model_edgetpu.tflite",
        experimental_delegates=[load_delegate("libedgetpu.so.1")]
    )
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    print("[‚úÖ] Edge TPU model loaded.")
except Exception as e:
    print(f"[‚ùå] Failed to load model or Edge TPU delegate: {e}")
    sys.exit(1)

# Labels must match training order
label_map = ['none', 'rock', 'paper', 'scissors']

app = Flask(__name__)
cap = setup_camera()
mp_hands = mp.solutions.hands.Hands(static_image_mode=False, max_num_hands=1)

if not cap.isOpened():
    print("‚ùå Failed to open camera.")
    sys.exit(1)
else:
    print("üé• Camera opened successfully.")

def generate_frames():
    prev_time = time.time()

    while True:
        ret, frame = cap.read()
        if not ret:
            print("[‚ö†Ô∏è] Frame capture failed.")
            continue

        frame = cv2.flip(frame, 1)
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = mp_hands.process(rgb_frame)

        gesture = "none"
        if results.multi_hand_landmarks:
            print("[üñê] Hand detected.")
            hand_landmarks = results.multi_hand_landmarks[0]
            landmarks = [[lm.x, lm.y] for lm in hand_landmarks.landmark]

            # Draw landmarks on frame
            draw_landmarks(frame, landmarks, frame.shape[1], frame.shape[0])

            # Prepare model input
            try:
                coords = np.array(landmarks).flatten().astype(np.float32)
                interpreter.set_tensor(input_details[0]['index'], [coords])
                interpreter.invoke()
                output_data = interpreter.get_tensor(output_details[0]['index'])

                # Get predicted gesture
                predicted_idx = np.argmax(output_data[0])
                gesture = label_map[predicted_idx]
                print(f"[ü§ñ] Prediction: {gesture}")
            except Exception as e:
                print(f"[‚ùå] Inference error: {e}")

        # Display gesture and FPS
        current_time = time.time()
        fps = 1.0 / (current_time - prev_time)
        prev_time = current_time

        cv2.putText(frame, f"Gesture: {gesture}", (10, 70),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 2)
        cv2.putText(frame, f"FPS: {fps:.2f}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 0), 2)

        success, buffer = cv2.imencode(".jpg", frame)
        if not success:
            print("[‚ö†Ô∏è] JPEG encoding failed.")
            continue

        yield (
            b"--frame\r\n"
            b"Content-Type: image/jpeg\r\n\r\n" + buffer.tobytes() + b"\r\n"
        )

@app.route("/")
def index():
    return (
        "<html><body>"
        "<h1>Gesture Recognition Stream</h1>"
        "<img src='/video' style='width:100%;height:auto;'/>"
        "</body></html>"
    )

@app.route("/video")
def video_feed():
    return Response(
        generate_frames(),
        mimetype="multipart/x-mixed-replace; boundary=frame"
    )

if __name__ == "__main__":
    print("[üåç] Flask server starting on port 80...")
    app.run(host="0.0.0.0", port=80, threaded=True, debug=False)
